# -*- coding: utf-8 -*-
"""suc_smolagents_gpu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sevQCoPR09aeNsyDO2j5uUgk9yE1S_jq
"""





"""https://sergiopaniego-promptist.hf.space/

https://huggingface.co/docs/smolagents/index

https://github.com/huggingface/smolagents
"""



!curl -fsSL https://ollama.com/install.sh | sh

import subprocess
import time

subprocess.Popen(["ollama", "serve"])
time.sleep(5) # Give Ollama server time to start

!ollama pull qwen3:14b
!ollama pull qwen3:8b

!pip install -r requirements.txt

#!pip install smolagents[toolkit] langchain langchain-community langchain-huggingface huggingface_hub transformers

import requests
from markdownify import markdownify
from requests.exceptions import RequestException
from smolagents import tool

@tool
def visit_webpage(url: str)->str:
  """
  Visits webpage at the given URL and returns its content as a markdown string
  Args:
    url: The URL of the webpage to visit
  Returns:
    The content of the webpage converted to Markdown, or an error message if the request doesn't work
  """

  try:
    response = requests.get(url)

    response.raise_for_status()

    markdown_content = markdownify(response.txt).strip()

    return markdown_content

  except RequestException as e:
    return f"Error fetching webpage {str(e)}"

  except Exception as e:
    return f"Error occurred {str(e)}"

requirements_content = """
dataclasses-json==0.6.7
ddgs==9.5.5
langchain-community==0.3.29
langchain-huggingface==0.3.1
lxml==6.0.1
markdownify==1.2.0
marshmallow==3.26.1
mypy-extensions==1.1.0
pillow==10.3.0
primp==0.15.0
requests==2.32.5
#smolagents==1.21.3
smolagents[toolkit]==1.21.3
typing-inspect==0.9.0
"""

with open("requirements.txt", "w") as f:
    f.write(requirements_content)

print("requirements.txt created successfully!")

with open("requirements.txt", "r") as f:
    print(f.read())

from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    WebSearchTool,
    OpenAIServerModel
)

reasoning_model_name = "qwen3:14b"
tool_model_name = "qwen3:8b"

def get_model(model_id):
  return OpenAIServerModel(
      model_id = model_id,
      api_base="http://localhost:11434/v1",
      api_key="ollama"
  )

tool_model = ToolCallingAgent(
    tools=[WebSearchTool(), visit_webpage],
    model= get_model(tool_model_name),
    max_steps=3,
    name = "web_search_agent",
    description = "Runs web searches for you"
)

reasoning_model = CodeAgent(
    tools=[],
    model= get_model(reasoning_model_name),
    managed_agents= [tool_model],
    additional_authorized_imports=["time", "numpy", "pandas"]
    )

reasoning_model.run("Please tell me about the sales of the latest Macbook Pro")

!pip install pillow==10.0.1

from smolagents import Tool, load_tool
#import PIL
import PIL.Image
@tool
def save_image(image: PIL.JpegImagePlugin.JpegImageFile) -> PIL.JpegImagePlugin.JpegImageFile:
  """
  Takes the image generated by the tool and saves it locally
  Args:
    image: The image genearted by the tool
  """
  image.save("out.png")
  return image

prompt_generator_tool = Tool.from_space("sergiopaniego/Promptist", name="generator_tool", description="Optimizes user input into model preferred input")
image_generation_tool = load_tool("m-ric/text-to-image", trust_remote_code=True)
image_generation_agent =  CodeAgent(tools=[prompt_generator_tool,image_generation_tool, save_image], model=get_model(reasoning_model_name),
                                    name="image_generation_agent", description="Generates and saves images from text prompt")

reasoning_model = CodeAgent(
    tools=[],
    model= get_model(reasoning_model_name),
    managed_agents= [tool_model, image_generation_agent],
    additional_authorized_imports=["time", "numpy", "pandas"]
)

from smolagents import Tool, load_tool
import PIL.Image
@tool
def save_image(image: PIL.Image.Image) -> PIL.Image.Image:
  """
  Takes the image generated by the tool and saves it locally
  Args:
    image: The image generated by the tool
  """
  image.save("out.png")
  return image

prompt_generator_tool = Tool.from_space("sergiopaniego/Promptist", name="generator_tool", description="Optimizes user input into model preferred input")
image_generation_tool = load_tool("m-ric/text-to-image", trust_remote_code=True)
image_generation_agent =  CodeAgent(tools=[prompt_generator_tool,image_generation_tool, save_image], model=get_model(reasoning_model_name),
                                    name="image_generation_agent", description="Generates and saves images from text prompt")

reasoning_model = CodeAgent(
    tools=[],
    model= get_model(reasoning_model_name),
    managed_agents= [tool_model, image_generation_agent],
    additional_authorized_imports=["time", "numpy", "pandas"]
)

image_generation_agent.run("A hi-res image of an independent repair shop repairing a MacBook display")

reasoning_model.run("A hi-res image of an independent repair shop repairing a MacBook display")

reasoning_model.run("Please tell me about the Framework Laptop 13! And generate an image of the Framework Laptop 13")

reasoning_model = CodeAgent(
    tools=[],
    model= get_model(reasoning_model_name),
    managed_agents= [tool_model, image_generation_agent],
    additional_authorized_imports=["time", "numpy", "pandas"],
    max_steps=3
)

