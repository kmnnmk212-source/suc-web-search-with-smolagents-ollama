# -*- coding: utf-8 -*-
"""suc_smolagent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rKutRJDMrqbPdYHP6_2hPvMtgcNTTx9T
"""









!pip install "smolagents[toolkit]"

!hf auth login



from transformers import AutoProcessor, AutoModelForCausalLM

processor = AutoProcessor.from_pretrained("google/functiongemma-270m-it", device_map="auto")
model = AutoModelForCausalLM.from_pretrained("google/functiongemma-270m-it", dtype="auto", device_map="auto")



from smolagents import TransformersModel

model = TransformersModel(
    model_id="Qwen/Qwen3-0.6B",
    max_new_tokens=64,
    device_map="auto"
)

!smolagent

!smolagent "Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7."  --model-type "InferenceClientModel" --model-id "Qwen/Qwen3-0.6B" --imports pandas numpy --tools web_search

# Run with direct prompt and options
smolagent "Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7."  --model-type "InferenceClientModel" --model-id "Qwen/Qwen3-Next-80B-A3B-Thinking" --imports pandas numpy --tools web_search

# Run in interactive mode (launches setup wizard when no prompt provided)
smolagent

from smolagents import CodeAgent, WebSearchTool, InferenceClientModel

model = InferenceClientModel()
agent = CodeAgent(tools=[WebSearchTool()], model=model, stream_outputs=True)

agent.run("How many seconds would it take for a leopard at full speed to run through Pont des Arts?")

!smolagent "Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7."  --model-id "Qwen/Qwen3-0.6B"  --tools web_search

!!smolagent --help

!# Run with direct prompt and options
smolagent "Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7."  --model-type "TransformersModel" --model-id "Qwen/Qwen3-Next-80B-A3B-Thinking" --imports pandas numpy --tools web_search

# Run in interactive mode (launches setup wizard when no prompt provided)
smolagent

from smolagents import TransformersModel

model = TransformersModel(
    model_id="Qwen/Qwen3-Next-80B-A3B-Thinking",
    max_new_tokens=4096,
    device_map="auto"
)

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="meta-llama/Llama-3.2-1B-Instruct")
messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe(messages)

!smolagent "Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7."  --model-type "TransformersModel" --model-id "Qwen/Qwen3-0.6B" --imports pandas numpy --tools web_search

!smolagent "what is 3 + 9 =?"  --model-type "TransformersModel" --model-id "Qwen/Qwen3-0.6B" --imports pandas numpy --tools web_search